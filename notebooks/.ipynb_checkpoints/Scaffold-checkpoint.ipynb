{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df7240cd-139b-4e57-8284-50202059ff72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/maxime/dev/python/sndx/.venv/bin/python3.12\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b106972-f8df-4c01-9b57-d8174a0d49f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "import sndx\n",
    "from sndx import Scrapper\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "from pyppeteer import launch\n",
    "from queue import Queue\n",
    "\n",
    "CHROMIUM_PATH = os.environ.get(\"CHROMIUM_PATH\")\n",
    "ROOT_URL = \"https://vod.catalogue-crc.org\"\n",
    "CATEGORIES_URL = \"https://vod.catalogue-crc.org/categorie.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e04dfad-21e1-4f68-a236-e0d9ce3e5d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening sink sndx-FBFXmg\n",
      "[RecordingScrapper-ZbkGiQ] Starting with profile [sndx-profile-1] and sink [sndx-FBFXmg]...\n",
      "Doing stuff\n",
      "Stuff finished\n",
      "[RecordingScrapper-ZbkGiQ] Terminating...\n",
      "Closing sink sndx-FBFXmg\n"
     ]
    }
   ],
   "source": [
    "profile = \"sndx-profile-1\"\n",
    "\n",
    "with sndx.Sink() as sink:\n",
    "    async with sndx.RecordingScrapper(profile, sink, headless=False) as scrapper:\n",
    "        print(\"Doing stuff\")\n",
    "        await asyncio.sleep(3)\n",
    "        print(\"Stuff finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "30114b9e-f536-4924-aeb4-70441ce2b6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_category_url(url):\n",
    "    return url.startswith(\"https://vod.catalogue-crc.org/categorie/\")\n",
    "\n",
    "def is_serie_url(url):\n",
    "    return url.startswith(\"https://vod.catalogue-crc.org/serie/\")\n",
    "\n",
    "def is_recording_url(url):\n",
    "    return url.startswith(\"https://vod.catalogue-crc.org/enregistrement/\")\n",
    "\n",
    "\n",
    "class Link:\n",
    "    def __init__(self, url, text):\n",
    "        self.url = url\n",
    "        self.text = text\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"[{self.text}]({self.url})\"\n",
    "\n",
    "\n",
    "class CategoryMetadata:\n",
    "    def __init__(self, url, title, categories):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.categories = categories\n",
    "\n",
    "\n",
    "class MetadataScrapper(Scrapper):\n",
    "    def __init__(self, profile_id, outdir, headless=True):\n",
    "        super().__init__(profile_id, headless)\n",
    "        self.outdir = Path(outdir).resolve()\n",
    "        self.categories_dir = self.outdir / \"categories\"\n",
    "        self.series_dir = self.outdir / \"series\"\n",
    "        self.recordings_dir = self.outdir / \"recordings\"\n",
    "        self.url_queue = None\n",
    "        self.visited = None\n",
    "\n",
    "    \n",
    "    async def open(self):\n",
    "        print(f\"[MetadataScrapper-{self.id}] Starting with profile [{self.profile_id}]...\")\n",
    "        self.browser = await launch(\n",
    "            headless=self.headless,\n",
    "            executablePath=CHROMIUM_PATH,\n",
    "            userDataDir=self.profile_path)\n",
    "        pages = await self.browser.pages()\n",
    "        self.page = pages[0]\n",
    "\n",
    "    async def close(self):\n",
    "        print(f\"[MetadataScrapper-{self.id}] Terminating...\")\n",
    "        await self.browser.close()\n",
    "\n",
    "\n",
    "    async def crawl(self):\n",
    "        await self.initialize()\n",
    "        \n",
    "        #while not self.queue.empty():\n",
    "        #    url = self.queue.get()\n",
    "        #    await self.process_resource_url(url)\n",
    "        #    self.queue.task_done()\n",
    "\n",
    "\n",
    "    async def initialize(self):\n",
    "        self.log(f\"Initializing...\")\n",
    "        self.queue = Queue()\n",
    "        self.visited = set()\n",
    "\n",
    "        self.categories_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.series_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.recordings_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        await self.page.goto(CATEGORIES_URL)\n",
    "        urls = await self.queue_resource_urls()   \n",
    "\n",
    "    \n",
    "    async def queue_resource_urls(self):\n",
    "        new = 0\n",
    "        for url in await self.extract_resource_urls():\n",
    "            if url not in self.visited:\n",
    "                self.queue.put(url)\n",
    "                new += 1\n",
    "        self.log(f\"Found {new} new resource URLs.\")\n",
    "\n",
    "    \n",
    "    async def process_resource_url(self, url):\n",
    "        self.log(f\"Scrapping [{url}]...\")\n",
    "        await self.page.goto(url)\n",
    "        \n",
    "        await self.queue_resource_urls()\n",
    "\n",
    "        if is_category_url(url):\n",
    "            await self.extract_category_metadata()\n",
    "        elif is_serie_url(url):\n",
    "            await self.extract_serie_metadata()\n",
    "        elif is_recording_url(url):\n",
    "            await self.extract_recording_metadata()\n",
    "\n",
    "\n",
    "    async def extract_category_metadata(self):\n",
    "        url = await self.page.evaluate('() => window.location.href')\n",
    "        breadcrumbs = await page.xpath(\"//ul[@class='uk-breadcrumb']//li\")\n",
    "        title = await scrapper.get_text(breadcrumbs[-1])\n",
    "        \n",
    "        categories = []\n",
    "        for elem in breadcrumbs[2:-1]:\n",
    "            a = (await elem.xpath(\".//a\"))[0]\n",
    "            url = await (await a.getProperty(\"href\")).jsonValue()\n",
    "            text = await (await a.getProperty(\"textContent\")).jsonValue()\n",
    "            categories.append(Link(url, text))\n",
    "\n",
    "        data = {\n",
    "            \"url\": url,\n",
    "            \"title\": title,\n",
    "            \"categories\": [{ \"url\": l.url, \"title\": l.text } for l in categories],\n",
    "        }\n",
    "\n",
    "    \n",
    "\n",
    "    async def extract_recording_metadata(self):\n",
    "        url = await self.page.evaluate('() => window.location.href')\n",
    "        category = await self.get_first(\"//h3[following-sibling::*[2][self::h1]]\")\n",
    "        title = await self.get_first(\"//h1[preceding-sibling::*[2][self::h3]]\")\n",
    "        subtitle = await self.get_first(\"//h2[preceding-sibling::*[3][self::h3]]\")\n",
    "\n",
    "        details = await self.page.xpath(\"//ul[@id='details']//dd\")\n",
    "        code = (await self.get_text(details[0])).strip()\n",
    "        date = (await self.get_text(details[1])).strip()\n",
    "        place = (await self.get_text(details[2])).strip()\n",
    "        authors = [s.strip() for s in (await self.get_text(details[3])).split(\"<br/>\")]\n",
    "        duration_str = (await self.get_text(details[4])).strip()\n",
    "\n",
    "        return RecordingMetadata(\n",
    "            url=url,\n",
    "            category=category,\n",
    "            title=title,\n",
    "            subtitle=subtitle,\n",
    "            code=code,\n",
    "            date=date,\n",
    "            place=place,\n",
    "            authors=authors,\n",
    "            duration=parse_duration(duration_str))\n",
    "        \n",
    "\n",
    "    async def extract_resource_urls(self):\n",
    "        urls = []\n",
    "        for elem in await page.xpath(\"//a\"):\n",
    "            urls.append(await (await elem.getProperty(\"href\")).jsonValue())\n",
    "            \n",
    "        return [u for u in urls if is_category_url(u) or is_serie_url(u) or is_recording_url(u)]\n",
    "\n",
    "\n",
    "    def log(self, msg):\n",
    "        print(f\"[MetadataScrapper-{self.id}] {msg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e7e2c302-e8c1-4003-aae1-ed38a0a09df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MetadataScrapper-Z5yM2g] Starting with profile [sndx-profile-1]...\n"
     ]
    }
   ],
   "source": [
    "profile = \"sndx-profile-1\"\n",
    "outdir = Path(\"~/.local/share/sndx\")\n",
    "\n",
    "scrapper = MetadataScrapper(profile, outdir, headless=False)\n",
    "await scrapper.open()\n",
    "browser = scrapper.browser\n",
    "page = scrapper.page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dbde3732-7226-4397-a1a2-aaec0322fe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MetadataScrapper-CqyiNQ] Terminating...\n"
     ]
    }
   ],
   "source": [
    "await scrapper.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c604e400-edde-4739-9153-8b35d7a4235c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MetadataScrapper-Z5yM2g] Initializing...\n",
      "[MetadataScrapper-Z5yM2g] Found 48 new resource URLs.\n"
     ]
    }
   ],
   "source": [
    "await scrapper.crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7931a0f1-6cd5-4d93-a5f2-f2b1eea9746e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CategoryMetadata at 0x7fffd9171130>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await scrapper.extract_category_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ff807946-3c2b-4279-97a2-6345c56c3e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queued: 48\n",
      "Visited: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Queued: {scrapper.queue.qsize()}\")\n",
    "print(f\"Visited: {len(scrapper.visited)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0eb4e086-5519-439d-950b-6f2ec79362d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[I - La Contre-Réforme dans l’actualité](https://vod.catalogue-crc.org/categorie/i-la-contre-reforme-dans-actualite.html)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breadcrumbs = await page.xpath(\"//ul[@class='uk-breadcrumb']//li\")\n",
    "current = breadcrumbs[-1]\n",
    "parents = breadcrumbs[2:-1]\n",
    "title = await scrapper.get_text(current)\n",
    "\n",
    "links = []\n",
    "for elem in parents:\n",
    "    a = (await elem.xpath(\".//a\"))[0]\n",
    "    url = await (await a.getProperty(\"href\")).jsonValue()\n",
    "    text = await (await a.getProperty(\"textContent\")).jsonValue()\n",
    "    links.append(Link(url, text))\n",
    "\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e2292181-abc9-459a-a535-0e9e2d06e28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MetadataScrapper-PPuNfg] Terminating...\n"
     ]
    }
   ],
   "source": [
    "await scrapper.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee8d1e9-1541-4118-b335-c0878c4d52c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
